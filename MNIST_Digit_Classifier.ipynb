{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrashiDave/MNIST-Digit-Classifier-CNN/blob/main/MNIST_Digit_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCDFDRu5aajq",
        "outputId": "504f3cf2-b0d4-4ce3-a0e8-1867e5762666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aNgPDR2Z9q5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7edpxeCqfQpx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLO7qQ_NjZNF"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Loads the MNIST dataset and splits it into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the MNIST dataset file.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (x_train, y_train, x_test, y_test)\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        x_train, y_train = f['x_train'], f['y_train']\n",
        "        x_test, y_test = f['x_test'], f['y_test']\n",
        "        return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_data, train_label, test_data, test_label = load_data('/content/drive/MyDrive/Colab Notebooks/DATA - 690 Deep Learning/mnist.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PS3AXXJjnX2"
      },
      "outputs": [],
      "source": [
        "X_train = (train_data/255 - 0.5)*2\n",
        "X_test = (test_data/255 - 0.5)*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Emh87AjqGs",
        "outputId": "cb1b9618-f6e9-403f-b772-5e747ed76f9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1xZ1B5Jjtfq"
      },
      "outputs": [],
      "source": [
        "train_data = X_train.reshape(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfoFMYLvjwdS",
        "outputId": "36adf4a8-a191-4812-d8a8-c95d3cdefef9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPkEcSSajy6h",
        "outputId": "1b576a11-4e10-467f-93f6-74951776dfc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoEuHHTSj1V1"
      },
      "outputs": [],
      "source": [
        "test_data = X_test.reshape(-1, 1, 28, 28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toZqrclZj3r4"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, train_label_raw, valid_label_raw = train_test_split(train_data, train_label, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YAXQ4NCj7CX",
        "outputId": "45449313-510f-4cb1-e930-38bc039ec614"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "eX_oHLyvj9wt",
        "outputId": "c4cfe57c-657c-4d73-f03b-c4ff3d322def"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHJZJREFUeJzt3X901PW95/HX5NeImkyMMZlEAgZQqALxSiHNqhRLSoi3Lgh1/dVzweuBKw2uSP1x01VR6960eK9aXYrnnlOh9gr+OCuwchVXgwlrDfQSpZT+yJI0SigkVHqYCUFCSD77B+u0A4n4HWbyTsLzcc73HDLzfef74evo029m+OJzzjkBANDPkqwXAAA4OxEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgIsV6ASfr6enRvn37lJ6eLp/PZ70cAIBHzjm1t7crPz9fSUl9X+cMuADt27dPBQUF1ssAAJyhlpYWDR8+vM/nB1yA0tPTJUnX6HqlKNV4NQAAr46rS+/rzch/z/uSsACtWLFCTz75pFpbW1VUVKTnnntOU6ZMOe3c5z92S1GqUnwECAAGnf9/h9HTvY2SkA8hvPLKK1q6dKmWLVumDz/8UEVFRSorK9OBAwcScTgAwCCUkAA99dRTWrBgge644w5dfvnlev7553XuuefqhRdeSMThAACDUNwDdOzYMdXX16u0tPQvB0lKUmlpqerq6k7Zv7OzU+FwOGoDAAx9cQ/Qp59+qu7ubuXm5kY9npubq9bW1lP2r6qqUiAQiGx8Ag4Azg7mfxC1srJSoVAosrW0tFgvCQDQD+L+Kbjs7GwlJyerra0t6vG2tjYFg8FT9vf7/fL7/fFeBgBggIv7FVBaWpomTZqk6urqyGM9PT2qrq5WSUlJvA8HABikEvLngJYuXap58+bpq1/9qqZMmaJnnnlGHR0duuOOOxJxOADAIJSQAN18883605/+pEceeUStra268sortWnTplM+mAAAOHv5nHPOehF/LRwOKxAIaJpmcScEABiEjrsu1WiDQqGQMjIy+tzP/FNwAICzEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAixXoBAM5eu3/8Nc8zDd9ekYCV9G7yj+72PJP77AcJWMnQxBUQAMAEAQIAmIh7gB599FH5fL6obdy4cfE+DABgkEvIe0BXXHGF3n333b8cJIW3mgAA0RJShpSUFAWDwUR8awDAEJGQ94B2796t/Px8jRo1Srfffrv27NnT576dnZ0Kh8NRGwBg6It7gIqLi7V69Wpt2rRJK1euVHNzs6699lq1t7f3un9VVZUCgUBkKygoiPeSAAADUNwDVF5erptuukkTJ05UWVmZ3nzzTR06dEivvvpqr/tXVlYqFApFtpaWlngvCQAwACX80wGZmZm67LLL1NjY2Ovzfr9ffr8/0csAAAwwCf9zQIcPH1ZTU5Py8vISfSgAwCAS9wDdd999qq2t1ccff6wPPvhAN954o5KTk3XrrbfG+1AAgEEs7j+C27t3r2699VYdPHhQF110ka655hpt3bpVF110UbwPBQAYxOIeoJdffjne3xLoN8kXXOB55g/3er/Tx/+a98+eZ5LlPM90y+d5pj/lJtd5nunhHspDBveCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcFc/DElH5hTHNPfphGTPM7+688cxHCnN80RSDP+/2KMezzNDUUNXd0xzmU1dcV4J/hpXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDB3bAx4PVcc6XnmZ8+/VRMxxqZ4v0u1f3lul/f5Hlm2BMZMR3rk+uHeZ759bxnYzqWV7uOOc8zFQ8vielYgX/fGtMcvhyugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFAPeH5cc9zwT601F/9zd6Xnm73bf6nmmuyrH88x579Z7njlyY7HnGUm6/W9rY5rrD3f8ap7nmbx/46aiAxFXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Gin7Vfd1Vnmd+Pun5GI7ki2FG+o9O7zcJTZre4n1G3meSxxR6nnnxmX/xPCNJ+Sn+mOa8uup/3ON55pJVTZ5nvN/OFv2BKyAAgAkCBAAw4TlAW7Zs0Q033KD8/Hz5fD6tX78+6nnnnB555BHl5eVp2LBhKi0t1e7du+O1XgDAEOE5QB0dHSoqKtKKFSt6fX758uV69tln9fzzz2vbtm0677zzVFZWpqNHj57xYgEAQ4fnDyGUl5ervLy81+ecc3rmmWf00EMPadasWZKkF198Ubm5uVq/fr1uueWWM1stAGDIiOt7QM3NzWptbVVpaWnksUAgoOLiYtXV1fU609nZqXA4HLUBAIa+uAaotbVVkpSbmxv1eG5ubuS5k1VVVSkQCES2goKCeC4JADBAmX8KrrKyUqFQKLK1tHj/8xEAgMEnrgEKBoOSpLa2tqjH29raIs+dzO/3KyMjI2oDAAx9cQ1QYWGhgsGgqqurI4+Fw2Ft27ZNJSUl8TwUAGCQ8/wpuMOHD6uxsTHydXNzs3bs2KGsrCyNGDFCS5Ys0RNPPKFLL71UhYWFevjhh5Wfn6/Zs2fHc90AgEHOc4C2b9+u6667LvL10qVLJUnz5s3T6tWr9cADD6ijo0MLFy7UoUOHdM0112jTpk0655xz4rdqAMCg5zlA06ZNk3Ouz+d9Pp8ef/xxPf7442e0MAxN/sYDnmfeap/oeWb8hb/2PCNJ5yV1ep5JCeaefqeT/PGm0Z5n/u4fNnme6a+bikrSH7q6PM9cXNvheeZ4a9vpd8KgYP4pOADA2YkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmPN8NGzgTx1v2ep55a9/lnmfuj/Fu2Necc9TzzIaNf/Y8c3Xax55nvpXu/fdUUv/3nmckqW7Sv3memfMf/+B5ZsQHv/I8g6GDKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3I8WAl3nXcc8zP9xQFNOx/jHb+80xn8z7IKZjeTVu4xLPM2N+3hXTsd5eFfA8M/KfejzPOM8TGEq4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPiccwPqfoDhcFiBQEDTNEspvlTr5WCQ6tg0Kqa56gmvxHkl8ZMUw/8vzpr4zZiO1X3wzzHNAZJ03HWpRhsUCoWUkZHR535cAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJlKsFwAkQpIvtnvsxnLDz/6S6kv2PHPw+rExHSvz53UxzQFeDNx/2wAAQxoBAgCY8BygLVu26IYbblB+fr58Pp/Wr18f9fz8+fPl8/mitpkzZ8ZrvQCAIcJzgDo6OlRUVKQVK1b0uc/MmTO1f//+yLZ27dozWiQAYOjx/CGE8vJylZeXf+E+fr9fwWAw5kUBAIa+hLwHVFNTo5ycHI0dO1aLFi3SwYMH+9y3s7NT4XA4agMADH1xD9DMmTP14osvqrq6Wj/60Y9UW1ur8vJydXd397p/VVWVAoFAZCsoKIj3kgAAA1Dc/xzQLbfcEvn1hAkTNHHiRI0ePVo1NTWaPn36KftXVlZq6dKlka/D4TARAoCzQMI/hj1q1ChlZ2ersbGx1+f9fr8yMjKiNgDA0JfwAO3du1cHDx5UXl5eog8FABhEPP8I7vDhw1FXM83NzdqxY4eysrKUlZWlxx57THPnzlUwGFRTU5MeeOABjRkzRmVlZXFdOABgcPMcoO3bt+u6666LfP35+zfz5s3TypUrtXPnTv3sZz/ToUOHlJ+frxkzZugHP/iB/H5//FYNABj0PAdo2rRpcq7vGz2+/fbbZ7Qg4GQpl4zwPDM92BDTsXrU43nmzk++6Xnm4mGHPM88kVPveebf/+mfPc9IUnnqfZ5nsl7gBqbwhnvBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETc/0pu4IukjLrE80zhK/s9z/xj9q88z0jSt34/x/NM6tyw55nf/ZcSzzNPLPN+N+z0pDTPM5J0uPyw55msF2I6FM5iXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSliljym0PPMyLXebyz6L/nve55Z236x5xlJSr3lqOeZ7kMhzzPZ/1rneea5/3qp55mKCxo8zwD9hSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFzM5d1e555un8/5OAlZxq+c+/HdNcwZ8+iPNKepd05eWeZ8ae83oCVgLY4QoIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUihlOEXxzT37Zz+uXHn3uOdnmeyf3M8ASvpnS/F+79GH/+3ZM8zM4Z1eJ7p8TxxQue+82KcBL48roAAACYIEADAhKcAVVVVafLkyUpPT1dOTo5mz56thoaGqH2OHj2qiooKXXjhhTr//PM1d+5ctbW1xXXRAIDBz1OAamtrVVFRoa1bt+qdd95RV1eXZsyYoY6Ov/xs+t5779Ubb7yh1157TbW1tdq3b5/mzJkT94UDAAY3T++ebtq0Kerr1atXKycnR/X19Zo6dapCoZB++tOfas2aNfrGN74hSVq1apW+8pWvaOvWrfra174Wv5UDAAa1M3oPKBQKSZKysrIkSfX19erq6lJpaWlkn3HjxmnEiBGqq6vr9Xt0dnYqHA5HbQCAoS/mAPX09GjJkiW6+uqrNX78eElSa2ur0tLSlJmZGbVvbm6uWltbe/0+VVVVCgQCka2goCDWJQEABpGYA1RRUaFdu3bp5ZdfPqMFVFZWKhQKRbaWlpYz+n4AgMEhpj+IunjxYm3cuFFbtmzR8OHDI48Hg0EdO3ZMhw4diroKamtrUzAY7PV7+f1++f3+WJYBABjEPF0BOee0ePFirVu3Tps3b1ZhYWHU85MmTVJqaqqqq6sjjzU0NGjPnj0qKSmJz4oBAEOCpyugiooKrVmzRhs2bFB6enrkfZ1AIKBhw4YpEAjozjvv1NKlS5WVlaWMjAzdfffdKikp4RNwAIAongK0cuVKSdK0adOiHl+1apXmz58vSXr66aeVlJSkuXPnqrOzU2VlZfrJT34Sl8UCAIYOn3POWS/ir4XDYQUCAU3TLKX4Uq2Xc1Zovec/xTT3ywd+HOeV9O5vVt7jeabgidhulOqbPMHzTFdVyPPMm1/5n55nkmL4zNCEX8z3PCNJhXf8wfNMT4f3m6ViaDruulSjDQqFQsrIyOhzP+4FBwAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMx/Y2oGFpy/vPA/mvQL/tmk+eZ3X9zRUzHuv+Ktz3P3Jr+x5iO5dX//uw8zzOjFu6J6Vjd3Nka/YArIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABDcjhRo/yY1prmtst+eZVF+y55lXxmz0PKMx3kckqct5/z01djnPM/N3zfM8k/Wt/+t5RgrFMAP0D66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3IwUuuzvt8c0t+o3Yz3PLMxsjOlYXl21dX5Mc+7DgOeZgv/+geeZLMVyY1FgaOEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwc1IEbONV1zgfUaTE7CSUw3Xb/rlOABixxUQAMAEAQIAmPAUoKqqKk2ePFnp6enKycnR7Nmz1dDQELXPtGnT5PP5ora77rorrosGAAx+ngJUW1uriooKbd26Ve+88466uro0Y8YMdXR0RO23YMEC7d+/P7ItX748rosGAAx+nj6EsGnTpqivV69erZycHNXX12vq1KmRx88991wFg8H4rBAAMCSd0XtAoVBIkpSVlRX1+EsvvaTs7GyNHz9elZWVOnLkSJ/fo7OzU+FwOGoDAAx9MX8Mu6enR0uWLNHVV1+t8ePHRx6/7bbbNHLkSOXn52vnzp168MEH1dDQoNdff73X71NVVaXHHnss1mUAAAYpn3POxTK4aNEivfXWW3r//fc1fPjwPvfbvHmzpk+frsbGRo0ePfqU5zs7O9XZ2Rn5OhwOq6CgQNM0Sym+1FiWBgAwdNx1qUYbFAqFlJGR0ed+MV0BLV68WBs3btSWLVu+MD6SVFxcLEl9Bsjv98vv98eyDADAIOYpQM453X333Vq3bp1qampUWFh42pkdO3ZIkvLy8mJaIABgaPIUoIqKCq1Zs0YbNmxQenq6WltbJUmBQEDDhg1TU1OT1qxZo+uvv14XXnihdu7cqXvvvVdTp07VxIkTE/IbAAAMTp7eA/L5fL0+vmrVKs2fP18tLS36zne+o127dqmjo0MFBQW68cYb9dBDD33hzwH/WjgcViAQ4D0gABikEvIe0OlaVVBQoNraWi/fEgBwluJecAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEynWCziZc06SdFxdkjNeDADAs+PqkvSX/573ZcAFqL29XZL0vt40XgkA4Ey0t7crEAj0+bzPnS5R/aynp0f79u1Tenq6fD5f1HPhcFgFBQVqaWlRRkaG0QrtcR5O4DycwHk4gfNwwkA4D845tbe3Kz8/X0lJfb/TM+CugJKSkjR8+PAv3CcjI+OsfoF9jvNwAufhBM7DCZyHE6zPwxdd+XyODyEAAEwQIACAiUEVIL/fr2XLlsnv91svxRTn4QTOwwmchxM4DycMpvMw4D6EAAA4OwyqKyAAwNBBgAAAJggQAMAEAQIAmBg0AVqxYoUuueQSnXPOOSouLtYvf/lL6yX1u0cffVQ+ny9qGzdunPWyEm7Lli264YYblJ+fL5/Pp/Xr10c975zTI488ory8PA0bNkylpaXavXu3zWIT6HTnYf78+ae8PmbOnGmz2ASpqqrS5MmTlZ6erpycHM2ePVsNDQ1R+xw9elQVFRW68MILdf7552vu3Llqa2szWnFifJnzMG3atFNeD3fddZfRins3KAL0yiuvaOnSpVq2bJk+/PBDFRUVqaysTAcOHLBeWr+74oortH///sj2/vvvWy8p4To6OlRUVKQVK1b0+vzy5cv17LPP6vnnn9e2bdt03nnnqaysTEePHu3nlSbW6c6DJM2cOTPq9bF27dp+XGHi1dbWqqKiQlu3btU777yjrq4uzZgxQx0dHZF97r33Xr3xxht67bXXVFtbq3379mnOnDmGq46/L3MeJGnBggVRr4fly5cbrbgPbhCYMmWKq6ioiHzd3d3t8vPzXVVVleGq+t+yZctcUVGR9TJMSXLr1q2LfN3T0+OCwaB78sknI48dOnTI+f1+t3btWoMV9o+Tz4Nzzs2bN8/NmjXLZD1WDhw44CS52tpa59yJf/apqanutddei+zzu9/9zklydXV1VstMuJPPg3POff3rX3f33HOP3aK+hAF/BXTs2DHV19ertLQ08lhSUpJKS0tVV1dnuDIbu3fvVn5+vkaNGqXbb79de/bssV6SqebmZrW2tka9PgKBgIqLi8/K10dNTY1ycnI0duxYLVq0SAcPHrReUkKFQiFJUlZWliSpvr5eXV1dUa+HcePGacSIEUP69XDyefjcSy+9pOzsbI0fP16VlZU6cuSIxfL6NOBuRnqyTz/9VN3d3crNzY16PDc3V7///e+NVmWjuLhYq1ev1tixY7V//3499thjuvbaa7Vr1y6lp6dbL89Ea2urJPX6+vj8ubPFzJkzNWfOHBUWFqqpqUnf//73VV5errq6OiUnJ1svL+56enq0ZMkSXX311Ro/frykE6+HtLQ0ZWZmRu07lF8PvZ0HSbrttts0cuRI5efna+fOnXrwwQfV0NCg119/3XC10QZ8gPAX5eXlkV9PnDhRxcXFGjlypF599VXdeeedhivDQHDLLbdEfj1hwgRNnDhRo0ePVk1NjaZPn264ssSoqKjQrl27zor3Qb9IX+dh4cKFkV9PmDBBeXl5mj59upqamjR69Oj+XmavBvyP4LKzs5WcnHzKp1ja2toUDAaNVjUwZGZm6rLLLlNjY6P1Usx8/hrg9XGqUaNGKTs7e0i+PhYvXqyNGzfqvffei/rrW4LBoI4dO6ZDhw5F7T9UXw99nYfeFBcXS9KAej0M+AClpaVp0qRJqq6ujjzW09Oj6upqlZSUGK7M3uHDh9XU1KS8vDzrpZgpLCxUMBiMen2Ew2Ft27btrH997N27VwcPHhxSrw/nnBYvXqx169Zp8+bNKiwsjHp+0qRJSk1NjXo9NDQ0aM+ePUPq9XC689CbHTt2SNLAej1Yfwriy3j55Zed3+93q1evdr/97W/dwoULXWZmpmttbbVeWr/63ve+52pqalxzc7P7xS9+4UpLS112drY7cOCA9dISqr293X300Ufuo48+cpLcU0895T766CP3ySefOOec++EPf+gyMzPdhg0b3M6dO92sWbNcYWGh++yzz4xXHl9fdB7a29vdfffd5+rq6lxzc7N799133VVXXeUuvfRSd/ToUeulx82iRYtcIBBwNTU1bv/+/ZHtyJEjkX3uuusuN2LECLd582a3fft2V1JS4kpKSgxXHX+nOw+NjY3u8ccfd9u3b3fNzc1uw4YNbtSoUW7q1KnGK482KALknHPPPfecGzFihEtLS3NTpkxxW7dutV5Sv7v55ptdXl6eS0tLcxdffLG7+eabXWNjo/WyEu69995zkk7Z5s2b55w78VHshx9+2OXm5jq/3++mT5/uGhoabBedAF90Ho4cOeJmzJjhLrroIpeamupGjhzpFixYMOT+J623378kt2rVqsg+n332mfvud7/rLrjgAnfuuee6G2+80e3fv99u0QlwuvOwZ88eN3XqVJeVleX8fr8bM2aMu//++10oFLJd+En46xgAACYG/HtAAIChiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAw8f8A6E3YLfqwhnMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(valid_data[2].reshape(28, 28))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LmZh2NEkDi6"
      },
      "outputs": [],
      "source": [
        "def label_to_one_hot(y, n_class):\n",
        "    \"\"\"Converts labels to one-hot encoding.\"\"\"\n",
        "    one_hot = np.zeros((y.shape[0], n_class))\n",
        "    for i in range(len(y)):\n",
        "        one_hot[i][int(y[i])] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "n_classes = 10\n",
        "train_label = label_to_one_hot(train_label_raw, n_classes)\n",
        "valid_label = label_to_one_hot(valid_label_raw, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9SZCZtIycXs"
      },
      "outputs": [],
      "source": [
        "def data_generator(data, label, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates batches of data and labels efficiently.\n",
        "\n",
        "    Args:\n",
        "        data (numpy.ndarray): The input data.\n",
        "        label (numpy.ndarray): The corresponding labels.\n",
        "        batch_size (int): The size of each batch.\n",
        "        shuffle (bool): Whether to shuffle the data before batching.\n",
        "\n",
        "    Yields:\n",
        "        Tuple of (data_batch, label_batch).\n",
        "    \"\"\"\n",
        "    num_samples = len(data)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        batch_indices = indices[start:end]\n",
        "\n",
        "        yield data[batch_indices], label[batch_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uo_V9nZPWJx"
      },
      "outputs": [],
      "source": [
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return int((input_size + 2*pad - filter_size) / stride) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVw7vSFDPVnq"
      },
      "outputs": [],
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBlj7oQkPViL"
      },
      "outputs": [],
      "source": [
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVZ_5IDKE1YI"
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"The base class for NN model layer.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, dout):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT4rJH-kuFsd"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.params = {}\n",
        "        self.params['W'] = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
        "        self.params['b'] = np.random.randn(output_dim)\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.params['W']) + self.params['b']\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        self.grads['W'] = np.dot(self.x.T, dout)\n",
        "        self.grads['b'] = np.sum(dout, axis=0)\n",
        "        return np.dot(dout, self.params['W'].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB5jjKTWuFsd"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.params = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        return dout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEJ-v_U2Psag"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, pad=0):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.params = {}\n",
        "        self.params['W'] = np.random.randn(\n",
        "            out_channels, in_channels, kernel_size[0], kernel_size[1]\n",
        "        ) / np.sqrt(in_channels * kernel_size[0] * kernel_size[1])\n",
        "        self.params['b'] = np.zeros(out_channels)\n",
        "\n",
        "        self.grads = {}\n",
        "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
        "        self.grads['b'] = np.zeros_like(self.params['b'])\n",
        "\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params['W'], self.params['b']\n",
        "        FN, C, FH, FW = W.shape\n",
        "        N, _, H, W_ = x.shape\n",
        "        out_h = (H + 2 * self.pad - FH) // self.stride + 1\n",
        "        out_w = (W_ + 2 * self.pad - FW) // self.stride + 1\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W = self.params['W']\n",
        "        FN, C, FH, FW = W.shape\n",
        "\n",
        "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
        "\n",
        "        self.grads['b'] = np.sum(dout, axis=0)\n",
        "        dW = np.dot(self.col.T, dout)\n",
        "        self.grads['W'] = dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDhXpAHXPsWY"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size=(2, 2), stride=2, pad=0):\n",
        "        self.pool_h = kernel_size[0]\n",
        "        self.pool_w = kernel_size[1]\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "        self.out_h = None\n",
        "        self.out_w = None\n",
        "        self.pool_size = self.pool_h * self.pool_w\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        self.out_h = (H + 2 * self.pad - self.pool_h) // self.stride + 1\n",
        "        self.out_w = (W + 2 * self.pad - self.pool_w) // self.stride + 1\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(N, C, self.out_h, self.out_w, self.pool_size)\n",
        "        col = col.transpose(0, 2, 3, 1, 4).reshape(-1, self.pool_size)\n",
        "\n",
        "        self.arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "\n",
        "        out = out.reshape(N, self.out_h, self.out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, out_h, out_w = dout.shape\n",
        "\n",
        "        dout_flat = dout.transpose(0, 2, 3, 1).reshape(-1)\n",
        "        dmax = np.zeros((dout_flat.size, self.pool_size))\n",
        "\n",
        "        dmax[np.arange(dout_flat.size), self.arg_max] = dout_flat\n",
        "\n",
        "        dmax = dmax.reshape(N, out_h, out_w, C, self.pool_size)\n",
        "        dmax = dmax.transpose(0, 3, 1, 2, 4).reshape(N * C * out_h * out_w, self.pool_size)\n",
        "\n",
        "        dx = col2im(dmax, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgHRckwnFBwQ"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb5zw88NFMXI"
      },
      "outputs": [],
      "source": [
        "class SoftmaxWithCrossEntropyLoss(Layer):\n",
        "    def __init__(self):\n",
        "        self.params = None\n",
        "\n",
        "    def forward(self, out, y):\n",
        "        '''\n",
        "            out: output of last fully connected layer\n",
        "            y: true label\n",
        "        '''\n",
        "\n",
        "        batch_size = out.shape[0]\n",
        "\n",
        "        out = softmax(out)\n",
        "\n",
        "        self.out = out\n",
        "        self.y = y\n",
        "\n",
        "        log_out = np.log(out + 1e-7)\n",
        "\n",
        "        loss = np.sum(-log_out * y)\n",
        "\n",
        "        return loss / batch_size, out\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "      batch_size = self.y.shape[0]\n",
        "      return (self.out - self.y) / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luv-IhsdQBAU"
      },
      "outputs": [],
      "source": [
        "# New Layer: Dropout\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params = None\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "        self.train_flg = True\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.train_flg = train_flg\n",
        "\n",
        "        if self.train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask / (1.0 - self.dropout_ratio)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if self.train_flg:\n",
        "            return dout * self.mask / (1.0 - self.dropout_ratio)\n",
        "        else:\n",
        "            return dout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTYgef2_QKFd"
      },
      "outputs": [],
      "source": [
        "# New Layer: BatchNormalization\n",
        "class BatchNormalization(Layer):\n",
        "    def __init__(self, gamma=1.0, beta=0.0, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.params = {}\n",
        "        self.params['gamma'] = gamma\n",
        "        self.params['beta'] = beta\n",
        "        self.momentum = momentum\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.xn = None\n",
        "        self.std = None\n",
        "\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if x.ndim == 4:\n",
        "            # For convolutional layers\n",
        "            N, C, H, W = x.shape\n",
        "            x_reshaped = x.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "            out = self._forward(x_reshaped, train_flg)\n",
        "            return out.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "        else:\n",
        "            # For fully connected layers\n",
        "            return self._forward(x, train_flg)\n",
        "\n",
        "    def _forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            D = x.shape[1]\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 1e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / np.sqrt(self.running_var + 1e-7)\n",
        "\n",
        "        out = self.params['gamma'] * xn + self.params['beta']\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim == 4:\n",
        "            # For convolutional layers\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "            dx = self._backward(dout)\n",
        "            return dx.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "        else:\n",
        "            # For fully connected layers\n",
        "            return self._backward(dout)\n",
        "\n",
        "    def _backward(self, dout):\n",
        "        dbeta = np.sum(dout, axis=0)\n",
        "        dgamma = np.sum(dout * self.xn, axis=0)\n",
        "\n",
        "        dxn = dout * self.params['gamma']\n",
        "        dxc = dxn / self.std\n",
        "\n",
        "        dstd = -np.sum(dxn * self.xc / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "\n",
        "        dxc += 2.0 * self.xc * dvar / self.batch_size\n",
        "        dmu = -np.sum(dxc, axis=0)\n",
        "        dx = dxc + dmu / self.batch_size\n",
        "\n",
        "        self.grads['gamma'] = dgamma\n",
        "        self.grads['beta'] = dbeta\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGGJ3LGHQAyp"
      },
      "outputs": [],
      "source": [
        "# New Layer: Global Average Pooling\n",
        "class GlobalAveragePooling(Layer):\n",
        "    def __init__(self):\n",
        "        self.params = None\n",
        "        self.grads = {}\n",
        "        self.x_shape = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "        N, C, H, W = x.shape\n",
        "        # Average over the height and width dimensions\n",
        "        out = np.mean(x, axis=(2, 3))\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C = dout.shape\n",
        "        _, _, H, W = self.x_shape\n",
        "\n",
        "        # Distribute the gradient equally across all the elements that were averaged\n",
        "        dx = np.ones(self.x_shape) * dout[:, :, np.newaxis, np.newaxis] / (H * W)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTySIZ5VQAwj"
      },
      "outputs": [],
      "source": [
        "class Optimizer(ABC):\n",
        "    \"\"\"The base class for optimizer.\"\"\"\n",
        "    def __init__(self, learning_rate, layers):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = layers\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paFwj0YBQAue"
      },
      "outputs": [],
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"SGD (Stochastic gradient descent) optimizer\"\"\"\n",
        "    def __init__(self, learning_rate, layers):\n",
        "        super().__init__(learning_rate, layers)\n",
        "\n",
        "    def update(self):\n",
        "        for i in range(len(self.layers)):\n",
        "            layer = self.layers[i]\n",
        "            if layer.params is not None:\n",
        "                for key in layer.params.keys():\n",
        "                    layer.params[key] -= self.learning_rate * layer.grads[key]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo1MGDs9QAsI"
      },
      "outputs": [],
      "source": [
        "# New Optimizer: Adam\n",
        "class Adam(Optimizer):\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate, layers, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        super().__init__(learning_rate, layers)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = {}  # First moment\n",
        "        self.v = {}  # Second moment\n",
        "        self.t = 0   # Timestep\n",
        "\n",
        "        # Initialize momentum and velocity\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.params is not None:\n",
        "                self.m[i] = {}\n",
        "                self.v[i] = {}\n",
        "                for key in layer.params.keys():\n",
        "                    self.m[i][key] = np.zeros_like(layer.params[key])\n",
        "                    self.v[i][key] = np.zeros_like(layer.params[key])\n",
        "\n",
        "    def update(self):\n",
        "        self.t += 1\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.params is not None:\n",
        "                for key in layer.params.keys():\n",
        "                    # Update biased first moment estimate\n",
        "                    self.m[i][key] = self.beta1 * self.m[i][key] + (1 - self.beta1) * layer.grads[key]\n",
        "                    # Update biased second raw moment estimate\n",
        "                    self.v[i][key] = self.beta2 * self.v[i][key] + (1 - self.beta2) * (layer.grads[key]**2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_corrected = self.m[i][key] / (1 - self.beta1**self.t)\n",
        "                    v_corrected = self.v[i][key] / (1 - self.beta2**self.t)\n",
        "\n",
        "                    # Update parameters\n",
        "                    layer.params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMrMSAaQQApC"
      },
      "outputs": [],
      "source": [
        "# CNN Model class\n",
        "class CNN:\n",
        "    def __init__(self):\n",
        "        # Initialize layers\n",
        "        self.layers = []\n",
        "\n",
        "        # Conv1: 1 -> 32 channels, 3x3 kernel\n",
        "        self.layers.append(Conv2d(1, 32, kernel_size=(3, 3), stride=1, pad=1))\n",
        "        self.layers.append(BatchNormalization())  # Add BatchNorm after first conv\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "\n",
        "        # Conv2: 32 -> 64 channels, 3x3 kernel\n",
        "        self.layers.append(Conv2d(32, 64, kernel_size=(3, 3), stride=1, pad=1))\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "\n",
        "        # Conv3: 64 -> 128 channels, 3x3 kernel\n",
        "        self.layers.append(Conv2d(64, 128, kernel_size=(3, 3), stride=1, pad=1))\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(Dropout(0.25))  # Add dropout layer as required\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.layers.append(GlobalAveragePooling())\n",
        "\n",
        "        # Fully connected layer: 128 -> 10 (for 10 digits)\n",
        "        self.layers.append(Linear(128, 10))\n",
        "\n",
        "        # Loss layer (will be used separately in the forward pass)\n",
        "        self.loss_layer = SoftmaxWithCrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        train_flg = t is not None\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "\n",
        "        if train_flg:\n",
        "            loss, output = self.loss_layer.forward(x, t)\n",
        "            return loss, output\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def backward(self):\n",
        "        dout = self.loss_layer.backward(dout=1)\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        return dout\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
        "                x = layer.forward(x, train_flg=False)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "\n",
        "\n",
        "        return softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mnist():\n",
        "    global train_data, train_label, valid_data, valid_label\n",
        "\n",
        "    model = CNN()\n",
        "\n",
        "    batch_size = 256\n",
        "    epochs = 10\n",
        "    learning_rate = 5e-3\n",
        "    optimizer = Adam(learning_rate, model.layers)\n",
        "\n",
        "    patience = 1\n",
        "    best_valid_acc = 0.0\n",
        "    wait = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(epochs):\n",
        "        train_data, train_label = shuffle(train_data, train_label)\n",
        "\n",
        "        train_loss, train_correct, total_batches = 0, 0, 0\n",
        "        for data_batch, label_batch in data_generator(train_data, train_label, batch_size):\n",
        "            loss, output = model.forward(data_batch, label_batch)\n",
        "            model.backward()\n",
        "            optimizer.update()\n",
        "\n",
        "            train_loss += loss\n",
        "            train_correct += np.sum(np.argmax(output, axis=1) == np.argmax(label_batch, axis=1))\n",
        "            total_batches += 1\n",
        "\n",
        "        train_loss /= total_batches\n",
        "        train_accuracy = train_correct / len(train_data)\n",
        "\n",
        "        # Validation phase\n",
        "        valid_loss, valid_correct, total_batches = 0, 0, 0\n",
        "        for data_batch, label_batch in data_generator(valid_data, valid_label, batch_size):\n",
        "            loss, output = model.forward(data_batch, label_batch)\n",
        "\n",
        "            valid_loss += loss\n",
        "            valid_correct += np.sum(np.argmax(output, axis=1) == np.argmax(label_batch, axis=1))\n",
        "            total_batches += 1\n",
        "\n",
        "        valid_loss /= total_batches\n",
        "        valid_accuracy = valid_correct / len(valid_data)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs} - '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
        "              f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "e8L38Bb4VaGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mnist(model):\n",
        "    global test_data, test_label\n",
        "\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_correct = 0\n",
        "    batch_size = 256\n",
        "\n",
        "    test_label_onehot = label_to_one_hot(test_label, n_classes)\n",
        "\n",
        "    for data_batch, label_batch in data_generator(test_data, test_label_onehot, batch_size):\n",
        "        _, output = model.forward(data_batch, label_batch)\n",
        "        test_correct += np.sum(np.argmax(output, axis=1) == np.argmax(label_batch, axis=1))\n",
        "\n",
        "    test_accuracy = test_correct / len(test_data)\n",
        "    print(f'Test accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "ao6S0YJaVeJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trained_model = train_mnist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdgCkExtVf6w",
        "outputId": "ed3fdb6d-42b5-4b08-d5d8-b3ffe32353e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10 - Train Loss: 0.9337, Train Acc: 0.7011, Valid Loss: 0.2522, Valid Acc: 0.9201\n",
            "Epoch 2/10 - Train Loss: 0.1733, Train Acc: 0.9486, Valid Loss: 0.1428, Valid Acc: 0.9563\n",
            "Epoch 3/10 - Train Loss: 0.1136, Train Acc: 0.9647, Valid Loss: 0.1344, Valid Acc: 0.9609\n",
            "Epoch 4/10 - Train Loss: 0.0873, Train Acc: 0.9725, Valid Loss: 0.0954, Valid Acc: 0.9703\n",
            "Epoch 5/10 - Train Loss: 0.0718, Train Acc: 0.9771, Valid Loss: 0.0897, Valid Acc: 0.9727\n",
            "Epoch 6/10 - Train Loss: 0.0564, Train Acc: 0.9814, Valid Loss: 0.0796, Valid Acc: 0.9763\n",
            "Epoch 7/10 - Train Loss: 0.0518, Train Acc: 0.9836, Valid Loss: 0.0753, Valid Acc: 0.9776\n",
            "Epoch 8/10 - Train Loss: 0.0447, Train Acc: 0.9855, Valid Loss: 0.0805, Valid Acc: 0.9776\n",
            "Epoch 9/10 - Train Loss: 0.0404, Train Acc: 0.9865, Valid Loss: 0.0868, Valid Acc: 0.9745\n",
            "Epoch 10/10 - Train Loss: 0.0331, Train Acc: 0.9893, Valid Loss: 0.0812, Valid Acc: 0.9771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "test_mnist(trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34HbVk59ddYu",
        "outputId": "0c3aa84c-0554-4f5c-ede9-963d4216b241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on test set...\n",
            "Test accuracy: 0.9754\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWMM9b5AmkTaNAP0ge3nQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}